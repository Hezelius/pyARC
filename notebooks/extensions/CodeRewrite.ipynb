{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%run CAR_creation.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def make_intervalfunc(minv, maxv, left_inclusivity, right_inclusivity):\n",
    "    def inner_func(value):\n",
    "        if greaterthan(value, minv, left_inclusivity) and lesserthan(value, maxv, right_inclusivity):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    return inner_func\n",
    "        \n",
    "def greaterthan(a, b, inclusivity):\n",
    "    if inclusivity:\n",
    "        if a >= b: return True\n",
    "    elif a > b: return True\n",
    "    \n",
    "    return False\n",
    "        \n",
    "def lesserthan(a, b, inclusivity):\n",
    "    if inclusivity:\n",
    "        if a <= b: return True\n",
    "    elif a < b: return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "class Interval:\n",
    "\n",
    "    def __init__(self, minval, maxval, left_inclusive, right_inclusive):\n",
    "        self.minval = minval\n",
    "        self.maxval = maxval\n",
    "        self.left_inclusive = left_inclusive\n",
    "        self.right_inclusive = right_inclusive\n",
    "        \n",
    "        \n",
    "        self.left_bracket = \"<\" if left_inclusive else \"(\"\n",
    "        self.right_bracket = \">\" if right_inclusive else \")\"\n",
    "        \n",
    "        self.__membership_func = np.vectorize(\n",
    "            make_intervalfunc(self.minval, self.maxval, self.left_inclusive, self.right_inclusive)\n",
    "        )\n",
    "            \n",
    "    \n",
    "    def __hash__(self):\n",
    "        return hash(repr(self))\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        return hash(self) == hash(other)\n",
    "            \n",
    "    def refit(self, vals):\n",
    "        \"\"\"refit values to a finer grid\n",
    "        \"\"\"\n",
    "        values = np.array(vals)\n",
    "        \n",
    "        mask = self.test_membership(values)\n",
    "        new_array = values[mask]\n",
    "\n",
    "        left, right = min(new_array), max(new_array)\n",
    "\n",
    "        return Interval(left, right, True, True)\n",
    "        \n",
    "            \n",
    "    def test_membership(self, value):\n",
    "        return self.__membership_func(value)\n",
    "        \n",
    "\n",
    "    def string(self):\n",
    "        return \"{}{};{}{}\".format(self.left_bracket, self.minval, self.maxval, self.right_bracket)\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return \"Interval[{}{};{}{}]\".format(self.left_bracket, self.minval, self.maxval, self.right_bracket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Interval[<1.2;2.3>]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "class IntervalReader():\n",
    "    \n",
    "    \n",
    "    interval_regex = re.compile(\"(<|\\()(\\d+(?:\\.(?:\\d)+)?);(\\d+(?:\\.(?:\\d)+)?)(\\)|>)\")\n",
    "    \n",
    "    \n",
    "    def __init__(self):\n",
    "        # opened interval brackets\n",
    "        self.__open_bracket = \"(\", \")\"\n",
    "        \n",
    "        # closed interval brackets\n",
    "        self.__closed_bracket = \"<\", \">\"\n",
    "        \n",
    "        # negative and positive infinity symbol,\n",
    "        # e.g. -inf, +inf\n",
    "        self.__infinity_symbol = \"-inf\", \"+inf\"\n",
    "        \n",
    "        # decimal separator, e.g. \".\", \",\"\n",
    "        self.__decimal_separator = \".\"\n",
    "        \n",
    "        # interval members separator\n",
    "        self.__members_separator = \";\"\n",
    "        \n",
    "        self.compile_reader()\n",
    "        \n",
    "        \n",
    "    def compile_reader(self):\n",
    "\n",
    "        left_bracket_open = re.escape(self.open_bracket[0])\n",
    "        left_bracket_closed = re.escape(self.closed_bracket[0])\n",
    "        \n",
    "        right_bracket_open = re.escape(self.open_bracket[1])\n",
    "        right_braket_closed = re.escape(self.closed_bracket[1])\n",
    "        \n",
    "        # e.g. (   <    |   \\(    ) \n",
    "        #      (   {}   |   {}    )\n",
    "        left_bracket_regex_string = \"({}|{})\".format(\n",
    "            left_bracket_open,\n",
    "            left_bracket_closed\n",
    "        )\n",
    "        \n",
    "        # e.g. (   >   |   \\)    ) \n",
    "        #      (   {}   |   {}    )\n",
    "        right_bracket_regex_string = \"({}|{})\".format(\n",
    "            right_bracket_open,\n",
    "            right_braket_closed\n",
    "        )\n",
    "        \n",
    "        # ((   \\d+  (?:  \\.   (?:\\d)+  )?   )|-inf)\n",
    "        # (   \\d+  (?:  {}   (?:\\d)+  )?   )\n",
    "        left_number_regex_string = \"(\\d+(?:{}(?:\\d)+)?|{})\".format(\n",
    "            re.escape(self.decimal_separator),\n",
    "            re.escape(self.infinity_symbol[0]),\n",
    "        )\n",
    "        \n",
    "        \n",
    "        # ((   \\d+  (?:  \\.   (?:\\d)+  )?   )|+inf)\n",
    "        # (   \\d+  (?:  {}   (?:\\d)+  )?   )\n",
    "        right_number_regex_string = \"(\\d+(?:{}(?:\\d)+)?|{})\".format(\n",
    "            re.escape(self.decimal_separator),\n",
    "            re.escape(self.infinity_symbol[1]),\n",
    "        )\n",
    "        \n",
    "        members_separator_regex = \"{}\".format(\n",
    "            re.escape(self.members_separator)\n",
    "        )\n",
    "        \n",
    "        \n",
    "        interval_regex_string = \"{}{}{}{}{}\".format(\n",
    "            left_bracket_regex_string,\n",
    "            left_number_regex_string,\n",
    "            members_separator_regex,\n",
    "            right_number_regex_string,\n",
    "            right_bracket_regex_string\n",
    "        )\n",
    "        \n",
    "        self.__interval_regex = re.compile(interval_regex_string)\n",
    "        \n",
    "        \n",
    "    def read(self, interval_string):\n",
    "        # returns array of results, take first member\n",
    "        args = self.__interval_regex.findall(interval_string)[0]\n",
    "        \n",
    "        left_bracket, minval, maxval, right_bracket = args\n",
    "        \n",
    "        left_inclusive = True if left_bracket == self.closed_bracket[0] else False\n",
    "        right_inclusive = True if right_bracket == self.closed_bracket[1] else False\n",
    "        \n",
    "        interval = Interval(\n",
    "            float(minval),\n",
    "            float(maxval),\n",
    "            left_inclusive,\n",
    "            right_inclusive\n",
    "        )\n",
    "        \n",
    "        return interval\n",
    "      \n",
    "        \n",
    "    # boilerplate getter/setter code    \n",
    "    \n",
    "    @property\n",
    "    def open_bracket(self):\n",
    "        return self.__open_bracket\n",
    "    \n",
    "    @open_bracket.setter\n",
    "    def open_bracket(self, val):\n",
    "        self.__open_bracket = val\n",
    "        return self\n",
    "    \n",
    "    @property\n",
    "    def closed_bracket(self):\n",
    "        return self.__closed_bracket\n",
    "    \n",
    "    @closed_bracket.setter\n",
    "    def closed_bracket(self, val):\n",
    "        self.__closed_bracket = val\n",
    "        return self\n",
    "        \n",
    "    @property\n",
    "    def infinity_symbol(self):\n",
    "        return self.__infinity_symbol\n",
    "    \n",
    "    @infinity_symbol.setter\n",
    "    def infinity_symbol(self, val):\n",
    "        self.__infinity_symbol = val\n",
    "        return self\n",
    "    \n",
    "    @property\n",
    "    def decimal_separator(self):\n",
    "        return self.__decimal_separator\n",
    "    \n",
    "    @decimal_separator.setter\n",
    "    def decimal_separator(self, val):\n",
    "        self.__decimal_separator = val\n",
    "        return self\n",
    "    \n",
    "    @property\n",
    "    def members_separator(self):\n",
    "        return self.__members_separator\n",
    "    \n",
    "    @members_separator.setter\n",
    "    def members_separator(self, val):\n",
    "        self.__members_separator = val\n",
    "        return self\n",
    "    \n",
    "    \n",
    "        \n",
    "interval_reader = IntervalReader()\n",
    "\n",
    "interval_reader.compile_reader()\n",
    "\n",
    "interval_reader.read(\"<1.2;2.3>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%run ../../main.py\n",
    "\n",
    "import copy\n",
    "\n",
    "class QuantitativeCAR:\n",
    "    \n",
    "    interval_reader = IntervalReader()\n",
    "    \n",
    "    def __init__(self, rule):\n",
    "        self.antecedent = self.__create_intervals_from_antecedent(rule.antecedent)\n",
    "        self.consequent = copy.copy(rule.consequent)\n",
    "        \n",
    "        self.confidence = rule.confidence\n",
    "        self.support = rule.support\n",
    "        self.rulelen = rule.rulelen\n",
    "        self.rid = rule.rid\n",
    "        \n",
    "        # property which indicates wheter the rule was extended or not\n",
    "        self.was_extended = False\n",
    "        # literal which extended the rule\n",
    "        self.extension_literal = None\n",
    "        \n",
    "        \n",
    "    def __create_intervals_from_antecedent(self, antecedent):\n",
    "        interval_antecedent = []\n",
    "        \n",
    "        for literal in antecedent:\n",
    "            attribute, value = literal\n",
    "            \n",
    "            interval = interval_reader.read(value)\n",
    "            \n",
    "            interval_antecedent.append((attribute, interval))\n",
    "        \n",
    "        \n",
    "        return self.__sort_antecedent(interval_antecedent)\n",
    "    \n",
    "    \n",
    "    def __sort_antecedent(self, antecedent):\n",
    "        return sorted(antecedent)\n",
    "    \n",
    "    \n",
    "    def update_properties(self, quant_dataframe):\n",
    "        \"\"\"updates rule properties using instance\n",
    "        of QuantitativeDataFrame\n",
    "        \n",
    "        properties:\n",
    "            support, confidence, rulelen\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        if type(quant_dataframe) != QuantitativeDataFrame:\n",
    "            raise Exception(\n",
    "                \"type of quant_dataframe must be QuantitativeDataFrame\"\n",
    "            )\n",
    "            \n",
    "        \n",
    "        support, confidence = quant_dataframe.calculate_rule_statistics(self)\n",
    "        \n",
    "        self.support = support\n",
    "        self.confidence = confidence\n",
    "        # length of antecedent + length of consequent\n",
    "        self.rulelen = len(self.antecedent) + 1\n",
    "        \n",
    "    \n",
    "    def copy(self):\n",
    "        return copy.deepcopy(self)\n",
    "        \n",
    "        \n",
    "    def __repr__(self):\n",
    "        ant = self.antecedent\n",
    "        ant_string_arr = [ key + \"=\" + val.string() for key, val in ant ]\n",
    "        ant_string = \"{\" + \",\".join(ant_string_arr) + \"}\"\n",
    "        \n",
    "        args = [\n",
    "            ant_string,\n",
    "            \"{\" + self.consequent.string() + \"}\",\n",
    "            self.support,\n",
    "            self.confidence,\n",
    "            self.rulelen,\n",
    "            self.rid\n",
    "        ]\n",
    "        \n",
    "        text = \"CAR {} => {} sup: {:.2f} conf: {:.2f} len: {}, id: {}\".format(*args)\n",
    "\n",
    "        return text\n",
    "    \n",
    "    \n",
    "    def __gt__(self, other):\n",
    "        \"\"\"\n",
    "        precedence operator. Determines if this rule\n",
    "        has higher precedence. Rules are sorted according\n",
    "        to their confidence, support, length and id.\n",
    "        \"\"\"\n",
    "        if (self.confidence > other.confidence):\n",
    "            return True\n",
    "        elif (self.confidence == other.confidence and\n",
    "              self.support > other.support):\n",
    "            return True\n",
    "        elif (self.confidence == other.confidence and\n",
    "              self.support == other.support and\n",
    "              self.rulelen < other.rulelen):\n",
    "            return True\n",
    "        elif(self.confidence == other.confidence and\n",
    "              self.support == other.support and\n",
    "              self.rulelen == other.rulelen and\n",
    "              self.rid < other.rid):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    \n",
    "    def __lt__(self, other):\n",
    "        \"\"\"\n",
    "        rule precedence operator\n",
    "        \"\"\"\n",
    "        return not self > other\n",
    "    \n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        print(\"self\", self)\n",
    "        print(\"other\", other)\n",
    "        print()\n",
    "        \n",
    "        return self.rid == other.rid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RuleCoverCache:\n",
    "    pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LiteralCache:\n",
    "    \"\"\"class which stores literals\n",
    "    and corresponding truth values\n",
    "    e.g. [\n",
    "        \"food=banana\": [True, True, False, False, True],\n",
    "        \"food=apple\" : [True, True, True, True, False]\n",
    "    ]\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.__cache = {}\n",
    "\n",
    "    def insert(self, literal, truth_values):\n",
    "        self.__cache[literal] = truth_values\n",
    "        \n",
    "    def get(self, literal):\n",
    "        return self.__cache[literal]\n",
    "        \n",
    "    def __contains__(self, literal):\n",
    "        \"\"\"function for using in\n",
    "        on LiteralCache object\n",
    "        \"\"\"\n",
    "        \n",
    "        return literal in self.__cache.keys()\n",
    "    \n",
    "    \n",
    "    \n",
    "cache = LiteralCache()\n",
    "\n",
    "cache.insert(\"food=apple\", np.array([True, True, False, False, True]))\n",
    "cache.insert(\"food=gingerbread\", np.array([False, False, False, False, True]))\n",
    "\n",
    "assert \"food=apple\" in cache\n",
    "assert \"blabla\" not in cache\n",
    "assert \"food=gingerbread\" in cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class QuantitativeDataFrame:\n",
    "    \n",
    "    def __init__(self, dataframe):\n",
    "        if type(dataframe) != pandas.DataFrame:\n",
    "            raise Exception(\"type of dataframe must be pandas.dataframe\")\n",
    "        \n",
    "        \n",
    "        self.__dataframe = dataframe\n",
    "        \n",
    "        # sorted and unique columns of the dataframe\n",
    "        # saved as a numpy array\n",
    "        self.__preprocessed_columns = self.__preprocess_columns(dataframe)\n",
    "        \n",
    "        \n",
    "        # literal cache for computing rule statistics\n",
    "        # - support and confidence\n",
    "        self.__literal_cache = LiteralCache()\n",
    "\n",
    "        # so that it doesn't have to be computed over and over\n",
    "        self.size = dataframe.index.size\n",
    "        \n",
    "        \n",
    "    @property\n",
    "    def dataframe(self):\n",
    "        return self.__dataframe\n",
    "    \n",
    "    \n",
    "    def column(self, colname):\n",
    "        return self.__preprocessed_columns[colname]\n",
    "    \n",
    "    \n",
    "    def mask(self, vals):\n",
    "        return self.__dataframe[vals]\n",
    "    \n",
    "    \n",
    "    def find_covered_by_antecedent_mask(self, antecedent):\n",
    "        \"\"\"\n",
    "        returns:\n",
    "            mask - an array of boolean values indicating which instances\n",
    "            are covered by antecedent\n",
    "        \"\"\"\n",
    "        \n",
    "        # todo: compute only once to make function faster\n",
    "        dataset_size = self.__dataframe.index.size\n",
    "        \n",
    "        for literal in antecedent:\n",
    "            attribute, interval = literal\n",
    "            \n",
    "            # the column that concerns the\n",
    "            # iterated attribute\n",
    "            # instead of pandas.Series, grab the ndarray\n",
    "            # using values attribute\n",
    "            relevant_column = self.__dataframe[[attribute]].values.reshape(dataset_size)\n",
    "            \n",
    "            # this tells us which instances satisfy the literal\n",
    "            current_mask = self.get_literal_coverage(literal, relevant_column)\n",
    "            \n",
    "            # add cummulated and current mask using logical AND\n",
    "            cummulated_mask &= current_mask\n",
    "    \n",
    "    \n",
    "    def find_covered_by_literal_mask(self, literal):\n",
    "        \"\"\"\n",
    "        returns:\n",
    "            mask - an array of boolean values indicating which instances\n",
    "            are covered by literal\n",
    "        \"\"\"\n",
    "        \n",
    "        for literal in rule.antecedent:\n",
    "            attribute, interval = literal\n",
    "            \n",
    "            # the column that concerns the\n",
    "            # iterated attribute\n",
    "            # instead of pandas.Series, grab the ndarray\n",
    "            # using values attribute\n",
    "            relevant_column = self.__dataframe[[attribute]].values.reshape(dataset_size)\n",
    "            \n",
    "            # this tells us which instances satisfy the literal\n",
    "            current_mask = self.get_literal_coverage(literal, relevant_column)\n",
    "            \n",
    "            # add cummulated and current mask using logical AND\n",
    "            cummulated_mask &= current_mask\n",
    "    \n",
    "    \n",
    "    def find_covered_by_rule_mask(self, rule):\n",
    "        \"\"\"\n",
    "        returns:\n",
    "            covered_by_antecedent_mask:\n",
    "                - array of boolean values indicating which\n",
    "                dataset rows satisfy antecedent\n",
    "                \n",
    "            covered_by_consequent_mask:\n",
    "                - array of boolean values indicating which\n",
    "                dataset rows satisfy conseqeunt\n",
    "        \"\"\"\n",
    "        \n",
    "        dataset_size = self.__dataframe.index.size\n",
    "        \n",
    "        # initialize a mask filled with True values\n",
    "        # it will get modified as futher literals get\n",
    "        # tested\n",
    "        \n",
    "        # for optimization - create cummulated mask once\n",
    "        # in constructor\n",
    "        cummulated_mask = np.array([True] * dataset_size)\n",
    "        \n",
    "        for literal in rule.antecedent:\n",
    "            attribute, interval = literal\n",
    "            \n",
    "            # the column that concerns the\n",
    "            # iterated attribute\n",
    "            # instead of pandas.Series, grab the ndarray\n",
    "            # using values attribute\n",
    "            relevant_column = self.__dataframe[[attribute]].values.reshape(dataset_size)\n",
    "            \n",
    "            # this tells us which instances satisfy the literal\n",
    "            current_mask = self.get_literal_coverage(literal, relevant_column)\n",
    "            \n",
    "            # add cummulated and current mask using logical AND\n",
    "            cummulated_mask &= current_mask\n",
    "            \n",
    "            \n",
    "        \n",
    "        instances_satisfying_antecedent_mask = cummulated_mask\n",
    "        instances_satisfying_consequent_mask = self.__get_consequent_coverage_mask(rule)\n",
    "        instances_satisfying_consequent_mask = instances_satisfying_consequent_mask.reshape(dataset_size)\n",
    "        \n",
    "        return instances_satisfying_antecedent_mask, instances_satisfying_consequent_mask\n",
    "        \n",
    "        \n",
    "    \n",
    "    def calculate_rule_statistics(self, rule):\n",
    "        \"\"\"calculates rule's confidence and\n",
    "        support using efficient numpy functions\n",
    "        \n",
    "        \n",
    "        returns:\n",
    "        --------\n",
    "        \n",
    "            support:\n",
    "                float\n",
    "            \n",
    "            confidence:\n",
    "                float\n",
    "        \"\"\"\n",
    "        \n",
    "        dataset_size = self.__dataframe.index.size\n",
    "        \n",
    "        # initialize a mask filled with True values\n",
    "        # it will get modified as futher literals get\n",
    "        # tested\n",
    "        \n",
    "        # for optimization - create cummulated mask once\n",
    "        # in constructor\n",
    "        cummulated_mask = np.array([True] * dataset_size)\n",
    "        \n",
    "        for literal in rule.antecedent:\n",
    "            attribute, interval = literal\n",
    "            \n",
    "            # the column that concerns the\n",
    "            # iterated attribute\n",
    "            # instead of pandas.Series, grab the ndarray\n",
    "            # using values attribute\n",
    "            relevant_column = self.__dataframe[[attribute]].values.reshape(dataset_size)\n",
    "            \n",
    "            # this tells us which instances satisfy the literal\n",
    "            current_mask = self.get_literal_coverage(literal, relevant_column)\n",
    "            \n",
    "            # add cummulated and current mask using logical AND\n",
    "            cummulated_mask &= current_mask\n",
    "            \n",
    "            \n",
    "        \n",
    "        \n",
    "        instances_satisfying_antecedent = self.__dataframe[cummulated_mask].index\n",
    "        instances_satisfying_antecedent_count = instances_satisfying_antecedent.size\n",
    "        \n",
    "        # using cummulated mask to filter out instances that satisfy consequent\n",
    "        # but do not satisfy antecedent\n",
    "        instances_satisfying_consequent_mask = self.__get_consequent_coverage_mask(rule)\n",
    "        instances_satisfying_consequent_mask = instances_satisfying_consequent_mask.reshape(dataset_size)\n",
    "        \n",
    "        instances_satisfying_consequent_and_antecedent = self.__dataframe[\n",
    "            instances_satisfying_consequent_mask & cummulated_mask\n",
    "        ].index\n",
    "        \n",
    "        instances_satisfying_consequent_and_antecedent_count = instances_satisfying_consequent_and_antecedent.size\n",
    "        instances_satisfying_consequent_count = self.__dataframe[instances_satisfying_consequent_mask].index.size\n",
    "        \n",
    "        # instances satisfying consequent both antecedent and consequent \n",
    "        support = instances_satisfying_consequent_and_antecedent_count / dataset_size\n",
    "        confidence = instances_satisfying_consequent_and_antecedent_count / instances_satisfying_antecedent_count\n",
    "        \n",
    "        return support, confidence\n",
    "    \n",
    "    \n",
    "    def __get_consequent_coverage_mask(self, rule):\n",
    "        consequent = rule.consequent\n",
    "        attribute, value = consequent\n",
    "        \n",
    "        class_column = self.__dataframe[[attribute]].values\n",
    "        \n",
    "        literal_key = \"{}={}\".format(attribute, value)\n",
    "\n",
    "        mask = []\n",
    "        \n",
    "        if literal_key in self.__literal_cache:\n",
    "            mask = self.__literal_cache.get(literal_key)\n",
    "        else:\n",
    "            mask = class_column == value\n",
    "        \n",
    "        return mask\n",
    "    \n",
    "    \n",
    "    def get_literal_coverage(self, literal, values):\n",
    "        \"\"\"returns mask which describes the instances that\n",
    "        satisfy the interval\n",
    "        \n",
    "        function uses cached results for efficiency\n",
    "        \"\"\"\n",
    "        \n",
    "        if type(values) != np.ndarray:\n",
    "            raise Exception(\"Type of values must be numpy.ndarray\")\n",
    "            \n",
    "        mask = []\n",
    "        \n",
    "        attribute, interval = literal\n",
    "        \n",
    "        literal_key = \"{}={}\".format(attribute, interval)\n",
    "        \n",
    "        # check if the result is already cached, otherwise\n",
    "        # calculate and save the result\n",
    "        if literal_key in self.__literal_cache:\n",
    "            mask = self.__literal_cache.get(literal_key)\n",
    "        else:\n",
    "            mask = interval.test_membership(values)\n",
    "            \n",
    "            self.__literal_cache.insert(literal_key, mask)\n",
    "            \n",
    "        # reshape mask into single dimension\n",
    "        mask = mask.reshape(values.size)\n",
    "            \n",
    "        return mask\n",
    "    \n",
    "    \n",
    "    def __preprocess_columns(self, dataframe):\n",
    "        \n",
    "        # covert to dict\n",
    "        # column -> list\n",
    "        # need to convert it to numpy array\n",
    "        dataframe_dict = dataframe.to_dict(orient=\"list\")\n",
    "        \n",
    "        dataframe_ndarray = {}\n",
    "        \n",
    "        \n",
    "        for column, value_list in dataframe_dict.items():\n",
    "            transformed_list = np.sort(np.unique(value_list))\n",
    "            dataframe_ndarray[column] = transformed_list\n",
    "            \n",
    "        return dataframe_ndarray\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "qds = QuantitativeDataFrame(movies_undiscr_txns)\n",
    "\n",
    "ds = movies_undiscr_txns\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'QuantitativeCAR'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-73-5604e5a797b1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[0mqrules\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m \u001b[0mQuantitativeCAR\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrules\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 249\u001b[1;33m \u001b[0mextended\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrule_ext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mqrules\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    250\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-73-5604e5a797b1>\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, rules)\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mcopied_rules\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m \u001b[0mrule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mrule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrules\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0mextended_rules\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__extend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrule\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mrule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcopied_rules\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mextended_rules\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-73-5604e5a797b1>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mcopied_rules\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m \u001b[0mrule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mrule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrules\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0mextended_rules\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__extend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrule\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mrule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcopied_rules\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mextended_rules\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-73-5604e5a797b1>\u001b[0m in \u001b[0;36m__extend\u001b[1;34m(self, rule)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__extend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m         \u001b[0mext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__extend_rule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mext\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-73-5604e5a797b1>\u001b[0m in \u001b[0;36m__extend_rule\u001b[1;34m(self, rule, min_improvement, min_conditional_improvement)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[0mcurrent_best\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrule\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m         \u001b[0mdirect_extensions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__get_extensions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-73-5604e5a797b1>\u001b[0m in \u001b[0;36m__get_extensions\u001b[1;34m(self, rule)\u001b[0m\n\u001b[0;32m    117\u001b[0m                 \u001b[0mcopied_rule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextended_literal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextended_literal\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m                 \u001b[0mextended_rules\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcopied_rule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mextended_rules\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'QuantitativeCAR'"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "\n",
    "class RuleExtender:\n",
    "    \n",
    "    def __init__(self, dataframe):\n",
    "    \n",
    "        if type(dataframe) != QuantitativeDataFrame:\n",
    "            raise Exception(\n",
    "                \"type of dataset must be pandas.DataFrame\"\n",
    "            )\n",
    "            \n",
    "        self.__dataframe = dataframe\n",
    "        \n",
    "        \n",
    "        \n",
    "    def transform(self, rules):\n",
    "        \n",
    "        copied_rules = [ rule.copy() for rule in rules ]\n",
    "        \n",
    "        extended_rules = [ self.__extend(rule) for rule in copied_rules ]\n",
    "        \n",
    "        return extended_rules\n",
    "    \n",
    "    \n",
    "    \n",
    "    def __extend(self, rule):\n",
    "        ext = self.__extend_rule(rule)\n",
    "        \n",
    "        return ext\n",
    "        \n",
    "    def __extend_rule(self, rule, min_improvement=0, min_conditional_improvement=-0.01):\n",
    "        \n",
    "        # check improvemnt argument ranges\n",
    "        \n",
    "        current_best = rule\n",
    "        direct_extensions = self.__get_extensions(rule)\n",
    "        \n",
    "        \n",
    "        while True:\n",
    "            extension_succesful = False\n",
    "            \n",
    "            for candidate in direct_extensions:\n",
    "                \n",
    "                candidate.update_properties(self.__dataframe)\n",
    "                \n",
    "                delta_confidence = candidate.confidence - current_best.confidence\n",
    "                delta_support = candidate.support - current_best.support\n",
    "                \n",
    "                \n",
    "                if self.__crisp_accept(delta_confidence, delta_support, min_improvement):\n",
    "                    current_best = candidate\n",
    "                    extension_succesful = True\n",
    "                    break\n",
    "                    \n",
    "                \n",
    "                if self.__conditional_accept(delta_confidence, min_conditional_improvement):\n",
    "                    enlargement = candidate\n",
    "                    \n",
    "                    while True:\n",
    "                        \n",
    "                        enlargement = self.get_beam_extensions(enlargement)\n",
    "                        \n",
    "                        if not enlargement:\n",
    "                            break\n",
    "                            \n",
    "                        candidate.update_properties(self.__dataframe)\n",
    "                        enlargement.update_properties(self.__dataframe)\n",
    "\n",
    "                        delta_confidence = enlargement.confidence - current_best.confidence\n",
    "                        delta_support = enlargement.support - current_best.support\n",
    "\n",
    "                        if self.__crisp_accept(delta_confidence, delta_support, min_improvement):\n",
    "                            current_best = enlargement\n",
    "                            extension_succesful = True\n",
    "                            \n",
    "                        elif self.__conditional_accept(delta_confidence, min_conditional_improvement):\n",
    "                            continue\n",
    "                        \n",
    "                        else:\n",
    "                            break\n",
    "            \n",
    "            \n",
    "                    if extension_succesful == True:\n",
    "                        break\n",
    "                        \n",
    "\n",
    "                else:\n",
    "                    # continue to next candidate\n",
    "                    continue\n",
    "           \n",
    "        \n",
    "            if extension_succesful == False:\n",
    "                break\n",
    "                    \n",
    "        return current_best\n",
    "        \n",
    "        \n",
    "    def __get_extensions(self, rule):\n",
    "        extended_rules = set()\n",
    "        \n",
    "        for literal in rule.antecedent:\n",
    "            attribute, interval = literal\n",
    "            \n",
    "            neighborhood = self.__get_direct_extensions(literal)\n",
    "            \n",
    "            for extended_literal in neighborhood:\n",
    "                # copy the rule so the extended literal\n",
    "                # can replace the default literal\n",
    "                copied_rule = rule.copy()\n",
    "                \n",
    "                # find the index of the literal\n",
    "                # so that it can be replaced\n",
    "                current_literal_index = copied_rule.antecedent.index(literal)\n",
    "                \n",
    "                copied_rule.antecedent[current_literal_index] = extended_literal\n",
    "                copied_rule.was_extended = True\n",
    "                copied_rule.extended_literal = extended_literal\n",
    "                \n",
    "                extended_rules.add(copied_rule)\n",
    "                \n",
    "        return extended_rules\n",
    "            \n",
    "    \n",
    "    def __get_direct_extensions(self, literal):\n",
    "        \"\"\"\n",
    "        ensure sort and unique\n",
    "        before calling functions\n",
    "        \"\"\"\n",
    "        \n",
    "        attribute, interval = literal\n",
    "        \n",
    "        vals = self.__dataframe.column(attribute)\n",
    "        vals_len = vals.size\n",
    "\n",
    "        mask = interval.test_membership(vals)\n",
    "\n",
    "        # indices of interval members\n",
    "        # we want to extend them \n",
    "        # once to the left\n",
    "        # and once to the right\n",
    "        # bu we have to check if resulting\n",
    "        # indices are not larger than value size\n",
    "        member_indexes = np.where(mask)[0]\n",
    "\n",
    "        first_index = member_indexes[0]\n",
    "        last_index = member_indexes[-1]\n",
    "\n",
    "        first_index_modified = first_index - 1\n",
    "        last_index_modified = last_index + 1\n",
    "        \n",
    "        no_left_extension = False\n",
    "        no_right_extension = False\n",
    "\n",
    "        if first_index_modified < 0:\n",
    "            no_left_extension = True\n",
    "\n",
    "        # if last_index_modified is larger than\n",
    "        # available indices\n",
    "        if last_index_modified > vals_len - 1:\n",
    "            no_right_extension = True\n",
    "\n",
    "\n",
    "        new_left_bound = interval.minval\n",
    "        new_right_bound = interval.maxval\n",
    "\n",
    "        if not no_left_extension:\n",
    "            new_left_bound = vals[first_index_modified]\n",
    "\n",
    "        if not no_right_extension:\n",
    "            new_right_bound = vals[last_index_modified]\n",
    "\n",
    "\n",
    "        # prepare return values\n",
    "        extensions = []\n",
    "\n",
    "        if not no_left_extension:\n",
    "            extension = new_left_bound, interval.maxval\n",
    "            \n",
    "            # when values are [1, 2, 3, 3, 4, 5]\n",
    "            # and the corresponding interval is (2, 4)\n",
    "            # instead of resulting interval being (1, 4)\n",
    "            \n",
    "            temp_interval = Interval(\n",
    "                new_left_bound,\n",
    "                interval.maxval,\n",
    "                True,\n",
    "                interval.right_inclusive\n",
    "            )\n",
    "\n",
    "            extensions.append((attribute, temp_interval))\n",
    "\n",
    "        if not no_right_extension:\n",
    "            extensoin = interval.minval, new_right_bound\n",
    "\n",
    "            temp_interval = Interval(\n",
    "                interval.minval,\n",
    "                new_right_bound,\n",
    "                interval.left_inclusive,\n",
    "                True\n",
    "            )\n",
    "\n",
    "            extensions.append((attribute, temp_interval))\n",
    "\n",
    "        return extensions\n",
    "        \n",
    "    \n",
    "    # make private\n",
    "    def get_beam_extensions(self, rule):\n",
    "        if not rule.was_extended:\n",
    "            return None\n",
    "\n",
    "        # literal which extended the rule\n",
    "        literal = rule.extended_literal\n",
    "        \n",
    "        extended_literal = self.__get_direct_extensions(literal)\n",
    "        \n",
    "        if not extended_literal:\n",
    "            return None\n",
    "        \n",
    "        copied_rule = rule.copy()\n",
    "        \n",
    "        literal_index = copied_rule.antecedent.index(literal)\n",
    "        \n",
    "        # so that literal is not an array\n",
    "        copied_rule.antecedent[literal_index] = extended_literal[0]\n",
    "        copied_rule.was_extended = True\n",
    "        copied_rule.extended_literal = extended_literal[0]\n",
    "        \n",
    "        return copied_rule\n",
    "\n",
    "    \n",
    "    \n",
    "    def __crisp_accept(self, delta_confidence, delta_support, min_improvement):\n",
    "        if delta_confidence >= min_improvement and delta_support > 0:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    def __conditional_accept(self, delta_conf, min_improvement):\n",
    "        if delta_conf >= min_improvement:\n",
    "            return True\n",
    "        \n",
    "        \n",
    "        \n",
    "rule_ext = RuleExtender(qds)      \n",
    "\n",
    "qrules = [ QuantitativeCAR(r) for r in rules ]\n",
    "\n",
    "extended = rule_ext.transform(qrules)\n",
    "\n",
    "\n",
    "for qr in qrules:\n",
    "    qr.update_properties(qds)\n",
    "\n",
    "[ print(qr) for qr in qrules ]\n",
    "print()\n",
    "print()\n",
    "[ print(ext) for ext in extended ]\n",
    "\n",
    "qds.calculate_rule_statistics(qrules[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RuleRefitter:\n",
    "    \"\"\"Refits the rule to a finer grid\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    def __init__(self, quantitative_dataframe):\n",
    "        self.__dataframe = quantitative_dataframe\n",
    "        \n",
    "        \n",
    "    def transform(self, rules):\n",
    "        copied_rules = [ rule.copy() for rule in rules  ]\n",
    "        refitted = [ self.__refit(rule) for rule in copied_rules ]\n",
    "        \n",
    "        return refitted\n",
    "        \n",
    "    def __refit(self, rule):\n",
    "        \"\"\"refits a single rule\n",
    "        \"\"\"\n",
    "\n",
    "        for idx, literal in enumerate(rule.antecedent):\n",
    "            attribute, interval = literal\n",
    "        \n",
    "            current_attribute_values = self.__dataframe.column(attribute)\n",
    "\n",
    "            refitted_interval = interval.refit(current_attribute_values)\n",
    "\n",
    "            rule.antecedent[idx] = attribute, refitted_interval\n",
    "            \n",
    "            \n",
    "        return rule\n",
    "            \n",
    "    \n",
    "    \n",
    "            \n",
    "    \n",
    "    \n",
    "rule_refitter = RuleRefitter(qds)\n",
    "\n",
    "rule_refitter.transform(qrules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RuleLiteralPruner:\n",
    "    \n",
    "    def __init__(self, quantitative_dataframe):\n",
    "        self.__dataframe = quantitative_dataframe\n",
    "        \n",
    "        \n",
    "    def transform(self, rules):\n",
    "        copied_rules = [ rule.copy() for rule in rules  ]\n",
    "        trimmed = [ self.__trim(rule) for rule in copied_rules ]\n",
    "        \n",
    "        return trimmed\n",
    "    \n",
    "    \n",
    "    def produce_combinations(self, array):\n",
    "        arr_len = len(array)\n",
    "    \n",
    "        for i in range(arr_len):\n",
    "            combination = array[0:i] + array[i+1:arr_len]\n",
    "        \n",
    "            yield combination\n",
    "    \n",
    "    \n",
    "    def __trim(self, rule):\n",
    "        if type(rule) != QuantitativeCAR:\n",
    "            raise Exception(\"type of rule must be QuantClassAssociationRule\")\n",
    "\n",
    "            \n",
    "        attr_removed = False\n",
    "    \n",
    "        literals = rule.antecedent\n",
    "        consequent = rule.consequent\n",
    "        \n",
    "        rule.update_properties(self.__dataframe)\n",
    "        \n",
    "        dataset_len = self.__dataframe.size\n",
    "\n",
    "        if len(literals) < 1:\n",
    "            return rule\n",
    "\n",
    "        while True:\n",
    "            for literals_combination in self.produce_combinations(literals):\n",
    "                if not literals_combination:\n",
    "                    continue\n",
    "                    \n",
    "                copied_rule = rule.copy()\n",
    "                \n",
    "                copied_rule.antecedent = literals_combination\n",
    "                copied_rule.update_properties(self.__dataframe)\n",
    "\n",
    "                if copied_rule.confidence > rule.confidence:\n",
    "                    rule.support = copied_rule.support\n",
    "                    rule.confidence = copied_rule.confidence\n",
    "                    rule.rulelen = copied_rule.rulelen\n",
    "                    \n",
    "                    rule.antecedent = copied_rule.antecedent\n",
    "\n",
    "                    attr_removed = True\n",
    "                    \n",
    "                    break\n",
    "                    \n",
    "                else:\n",
    "                    attr_removed = False\n",
    "\n",
    "            if attr_removed == False:\n",
    "                break\n",
    "                \n",
    "                \n",
    "        return rule\n",
    "    \n",
    "    \n",
    "literal_pruner = RuleLiteralPruner(qds)\n",
    "\n",
    "literal_pruner.transform(qrules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RuleTrimmer:\n",
    "    \"\"\"Trims the rule\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    def __init__(self, quantitative_dataframe):\n",
    "        self.__dataframe = quantitative_dataframe\n",
    "        \n",
    "        \n",
    "    def transform(self, rules):\n",
    "        copied_rules = [ rule.copy() for rule in rules  ]\n",
    "        trimmed = [ self.__trim(rule) for rule in copied_rules ]\n",
    "        \n",
    "        return trimmed\n",
    "    \n",
    "    \n",
    "    def __trim(self, rule):\n",
    "        if type(rule) != QuantitativeCAR:\n",
    "            raise Exception(\"type of rule must be QuantClassAssociationRule\")\n",
    "\n",
    "            \n",
    "        covered_by_antecedent_mask, covered_by_consequent_mask = self.__dataframe.find_covered_by_rule_mask(rule)\n",
    "        \n",
    "        covered_by_rule_mask = covered_by_antecedent_mask & covered_by_consequent_mask\n",
    "        \n",
    "        # instances covered by rule\n",
    "        correctly_covered_by_r = self.__dataframe.mask(covered_by_rule_mask)\n",
    "        \n",
    "        antecedent = rule.antecedent\n",
    "\n",
    "        for idx, literal in enumerate(antecedent):\n",
    "\n",
    "            attribute, interval = literal\n",
    "            \n",
    "            current_column = correctly_covered_by_r[[attribute]].values\n",
    "            current_column_unique = np.unique(current_column)\n",
    "\n",
    "            if not current_column.any():\n",
    "                continue\n",
    "\n",
    "            minv = np.asscalar(min(current_column))\n",
    "            maxv = np.asscalar(max(current_column))\n",
    "\n",
    "            new_interval = Interval(minv, maxv, True, True)\n",
    "\n",
    "            antecedent[idx] = attribute, new_interval\n",
    "\n",
    "        return rule\n",
    "    \n",
    "    \n",
    "    \n",
    "rule_trimmer = RuleTrimmer(qds)\n",
    "\n",
    "\n",
    "[ print(r) for r in qrules ]\n",
    "\n",
    "print()\n",
    "\n",
    "rule_trimmer.transform(qrules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "from scipy import stats\n",
    "\n",
    "class RulePostPruner:\n",
    "    \n",
    "    def __init__(self, quantitative_dataset):\n",
    "        self.__dataframe = quantitative_dataset\n",
    "        \n",
    "        \n",
    "    def transform(self, rules):\n",
    "        copied_rules = [ rule.copy() for rule in rules ]\n",
    "\n",
    "        self.prune(copied_rules)\n",
    "        \n",
    "        \n",
    "    def preprocess_dataframe(self):\n",
    "        return self.__dataframe.dataframe.index.values\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def get_most_frequent_class(self):\n",
    "        \"\"\" \n",
    "        requires class column to be the last in dataframe\n",
    "        \n",
    "        gets the most frequent class from dataset\n",
    "        - naive implementation\n",
    "        \"\"\"\n",
    "        \n",
    "        index_counts, possible_classes = pd.factorize(self.__dataframe.dataframe.iloc[:, -1].values)\n",
    "        counts = np.bincount(index_counts)\n",
    "        counts_max = counts.max()\n",
    "        most_frequent_classes = possible_classes[counts == counts_max]\n",
    "        \n",
    "        # return only one\n",
    "        return most_frequent_classes[0], counts_max\n",
    "    \n",
    "    \n",
    "    def get_most_frequent_from_numpy(self, ndarray):\n",
    "        \"\"\"gets a mode from numpy array\n",
    "        \"\"\"\n",
    "        unique, pos = np.unique(a, return_inverse=True) \n",
    "        counts = np.bincount(pos)                  \n",
    "        maxpos = counts.argmax()                      \n",
    "\n",
    "        return (unique[maxpos], counts[maxpos])\n",
    "        \n",
    "    \n",
    "    def find_covered(self):\n",
    "        pass\n",
    "        \n",
    "        \n",
    "    def prune(self, rules):\n",
    "        \n",
    "        dataset = self.preprocess_dataframe()\n",
    "        dataset_len = dataset.size\n",
    "        dataset_mask = [ True ] * dataset_len\n",
    "        \n",
    "        cutoff_rule = rules[-1]\n",
    "        cutoff_class, cutoff_class_count = self.get_most_frequent_class()\n",
    "        \n",
    "        default_class = cutoff_class\n",
    "\n",
    "        total_errors_without_default = 0\n",
    "        \n",
    "        lowest_total_error = dataset_len - cutoff_class_count\n",
    "        \n",
    "        # implement comparators\n",
    "        rules.sort(reverse=True)\n",
    "        \n",
    "        rules_pruned = []\n",
    "        \n",
    "        for rule in rules:\n",
    "            covered_antecedent, covered_consequent = self.__dataframe.find_covered_by_rule_mask(rule)\n",
    "\n",
    "            correctly_covered = covered_antecedent & covered_consequent\n",
    "            \n",
    "            # dataset -= covered_antecedent\n",
    "            dataset_mask = dataset_mask & covered_antecedent\n",
    "                        \n",
    "            \n",
    "            if any(correctly_covered):\n",
    "                misclassified = np.sum(covered_antecedent) - np.sum(correctly_covered)\n",
    "                \n",
    "                \n",
    "                total_errors_without_default += misclassified\n",
    "                \n",
    "                modified_dataset = dataset[dataset_mask]\n",
    "                \n",
    "                \n",
    "            \n",
    "                default_class, default_class_count = self.get_most_frequent_from_numpy(modified_dataset)\n",
    "                \n",
    "        \n",
    "                # don't forget to update dataset length\n",
    "                default_rule_error = dataset_len - default_class_count\n",
    "                total_errors_with_default = default_rule_error + total_errors_without_default\n",
    "                \n",
    "                if total_errors_with_default < lowest_total_error:\n",
    "                    cutoff_rule = rule\n",
    "                    lowest_total_error = total_errors_with_default\n",
    "                    cutoff_class = default_class\n",
    "                \n",
    "                \n",
    "        \n",
    "        # remove all rules below cutoff rule\n",
    "        index_to_cut = rules.index(cutoff_rule)\n",
    "        rules_pruned = rules[:index_to_cut+1]\n",
    "        \n",
    "        # append new default rule\n",
    "        empty_rule = cutoff_rule.copy()\n",
    "        empty_rule.antecedent = []\n",
    "        empty_rule.consequent = self.__dataframe.dataframe.columns[-1], cutoff_class\n",
    "        \n",
    "        \n",
    "        rules_pruned.append(empty_rule)\n",
    "        \n",
    "        return rules_pruned\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "rulepostpruner = RulePostPruner(qds)\n",
    "\n",
    "\n",
    "\n",
    "rulepostpruner.transform(qrules)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hash(frozenset([2, 2, 3]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
